{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multiple Object Tracking\n",
    "\n",
    "---\n",
    "\n",
    "Multi-Object Tracking (MOT) is a core visual ability that humans poses to perform kinetic tasks and coordinate other tasks. The AI community has recognized the importance of MOT via a series of competitions.\n",
    "\n",
    "The ability to reason even in the absence of perception input task was highlighted in Lecture 1 using a document camera and a canopy type of occlusion where an object moves below it. In this assignment, the object class is `ball` and the ability to reason over time will be demonstrated using Kalman Filters. There will be two cases of occlusion: occlusion by a different object and occlusion by the same object (typical case of the later is on tracking people in crowds).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Understand the problem and setup environment (20 points)\n",
    "\n",
    "The problem is best described using this explanatory video below of the raw source files of this assignment:\n",
    "\n",
    "1. Single object Tracking - ball.mp4\n",
    "2. Multiple object Tracking - multiple_ball.avi\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded both files and watched the video.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Object Detector (40 points)\n",
    "\n",
    "In this task you will use a CNN-based object detector to bound box all `ball` instances in each frame. Because the educational value is not object detection, you are allowed to use an object detector of your choice trained to distinguish the `ball` class. You are free to use a pre-trained model (eg on MS COCO that contains the class `sports ball` or train a model yourself. Ensure that you explain thoroughly the code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5s')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743.290405</td>\n",
       "      <td>48.343658</td>\n",
       "      <td>1141.756592</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>0.879861</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441.989624</td>\n",
       "      <td>437.336731</td>\n",
       "      <td>496.585083</td>\n",
       "      <td>710.036194</td>\n",
       "      <td>0.675119</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123.051117</td>\n",
       "      <td>193.238068</td>\n",
       "      <td>714.690796</td>\n",
       "      <td>719.771240</td>\n",
       "      <td>0.666693</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>978.989807</td>\n",
       "      <td>313.579468</td>\n",
       "      <td>1025.302856</td>\n",
       "      <td>415.526184</td>\n",
       "      <td>0.261517</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class    name\n",
       "0  743.290405   48.343658  1141.756592  720.000000    0.879861      0  person\n",
       "1  441.989624  437.336731   496.585083  710.036194    0.675119     27     tie\n",
       "2  123.051117  193.238068   714.690796  719.771240    0.666693      0  person\n",
       "3  978.989807  313.579468  1025.302856  415.526184    0.261517     27     tie"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = \"https://ultralytics.com/images/zidane.jpg\"\n",
    "results = model(im)\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "#### try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('ball.mp4') # open video with opencv library\n",
    "save_path = '/saved/'\n",
    "while vidcap.isOpened():\n",
    "  x,frame = vidcap.read() # read the video one frame at a time\n",
    "  # cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file \n",
    "  # cv2.resize(frame, (640, 640))  # resize frame to the size expected by the model\n",
    "  # input_tensor = torch.from_numpy(frame).permute(2,0,1).float().unsqueeze(0)\n",
    "  if x:\n",
    "    results = model(frame) # run it through yolov5\n",
    "    df = results.pandas()\n",
    "    # results.save(save_dir=save_path)\n",
    "    # results.print()\n",
    "    results.show()\n",
    "    # print(results.pred[results.pred[:, -1]==38])\n",
    "  else:\n",
    "    break\n",
    "  \n",
    "  print(results.boxes[0])\n",
    "  for box in boxes:\n",
    "    x1, y1, x2, y2 = box.detach().numpy().astype(int)\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "\n",
    "  cv2.imshow('frame',frame)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chang/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-14 Python-3.10.11 torch-2.0.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\chang\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# initialize models\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5s')\n",
    "\n",
    "cap = cv2.VideoCapture('ball.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.cv.CV_FOURCC(*'DIVX')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "out = cv2.VideoWriter('output.avi', -1, 30.0, (640,480))\n",
    "\n",
    "def printall(dict, frame, ind):\n",
    "    for i in range(ind,0,-1):\n",
    "        cv2.circle(frame, (dict[i][0]+50, dict[i][1]+50), 5, (0,0,255),-1)\n",
    "\n",
    "loc = {}\n",
    "frame_ind = 0\n",
    "\n",
    "model.classes = [32]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # frame = cv2.flip(frame,0)\n",
    "        # write the flipped frame\n",
    "        # cv2.circle(frame, (100, 100), 5, (0, 0, 255), -1)\n",
    "        detections = model(frame)\n",
    "        pred = detections.pandas().xyxy[0]\n",
    "\n",
    "        # print(type(pred[0]['xmin'][0]))\n",
    "        # print(pred[0]['xmin'][0])\n",
    "        # try:\n",
    "        #     if isinstance(pred[0]['xmin'][0],float): # frame successfully has data\n",
    "        #         pred = detections.pandas().xyxy\n",
    "        #         print(type(pred[0]['xmin']))\n",
    "        #         xmin = float(pred[0]['xmin'])\n",
    "        #         ymin = float(pred[0]['ymin'])\n",
    "        #         xmax = float(pred[0]['xmax'])\n",
    "        #         ymax = float(pred[0]['ymax'])\n",
    "        #         loc[frame_ind] = (xmin,ymin,xmax, ymax)\n",
    "        # except:\n",
    "        #     loc[frame_ind] = loc[frame_ind-1]\n",
    "        #     frame = cv2.rectangle(frame, (loc[frame_ind][0], loc[frame_ind][1]), (loc[frame_ind][2],loc[frame_ind][3]), color=(0,0,255))\n",
    "        #     out.write(frame)\n",
    "        #     printall(loc, frame, frame_ind)\n",
    "        #     pass\n",
    "\n",
    "        for ind, row in pred.iterrows():\n",
    "            if str(row['name']==\"sports ball\"):\n",
    "                x1= int(row['xmin'])\n",
    "                y1= int(row['ymin'])\n",
    "                x2= int(row['xmax'])\n",
    "                y2= int(row['ymax'])\n",
    "                loc[frame_ind] = (x1,y1,x2,y2)\n",
    "            else:\n",
    "                loc[frame_ind] = loc[frame_ind-1]\n",
    "                pass\n",
    "                print((row['xmin'],row['ymin'],row['xmax'],row['ymax']))\n",
    "        try:\n",
    "            frame = cv2.rectangle(frame, (loc[frame_ind][0], loc[frame_ind][1]), (loc[frame_ind][2],loc[frame_ind][3]), color=(0,0,255))\n",
    "            out.write(frame)\n",
    "            printall(loc, frame, frame_ind)\n",
    "            frame_ind += 1\n",
    "            #detections.show()\n",
    "        except:\n",
    "            frame_ind += 1\n",
    "            pass\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Tracker (40 points)\n",
    "---\n",
    "\n",
    "The detector outputs can be used to obtain the centroid(s) of the `ball` instances across time. You can assign a suitable starting state in the 1st frame of the video and obtain the predicted trajectory of the object during both visible and occluded frames. You need to superpose your predicted position of the object in each frame and the raw frame and store a sequence of all frames (generate a video). Ensure that you explain thoroughly the code.\n",
    "\n",
    "Please note that you can use the filterpy library to implement the Kalman filter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "1. Initalize state with position and velocity\n",
    "2. define state transition matrix (describes how it evolves over time)\n",
    "3. define measurement matrix\n",
    "4. define process noise covariance matrix\n",
    "5. for each time step, update the state estimate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from filterpy.kalman import KalmanFilter\n",
    "# from yolov5.detect import detect\n",
    "import utils\n",
    "#display = utils.notebook_init()\n",
    "\n",
    "cap = cv2.VideoCapture('ball.mp4')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps,(width, height))\n",
    "frame_ind = 0\n",
    "loc={}\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    detections = model(frame)\n",
    "    loc[frame_ind] = detections.pandas().xyxy[0]\n",
    "\n",
    "    pred = detections.pandas().xyxy[0]\n",
    "    for ind, row in pred.iterrows():\n",
    "        if str(row['name']==\"sports ball\"):\n",
    "            x1= int(row['xmin'])\n",
    "            # print(f'x1:{x1}')\n",
    "            y1= int(row['ymin'])\n",
    "            x2= int(row['xmax'])\n",
    "            y2= int(row['ymax'])\n",
    "            loc[frame_ind] = (x1,y1,x2,y2)\n",
    "            # print((row['xmin'],row['ymin'],row['xmax'],row['ymax']))\n",
    "    try:\n",
    "        frame = cv2.rectangle(frame, (loc[frame_ind][0], loc[frame_ind][1]), (loc[frame_ind][2],loc[frame_ind][3]), color=(0,0,255))\n",
    "        detections.show()\n",
    "    except:\n",
    "        frame_ind += 1\n",
    "        pass\n",
    "    # print(detections.xyxy[0][1].numpy())\n",
    "    cv2.imshow(\"image\",frame)\n",
    "    frame_ind += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chang/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-14 Python-3.10.11 torch-2.0.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\chang\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5s')\n",
    "model.classes=[32]\n",
    "\n",
    "cap = cv2.VideoCapture('ball.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.cv.CV_FOURCC(*'DIVX')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "out = cv2.VideoWriter('output.avi', -1, 30.0, (640,480))\n",
    "\n",
    "kf = KalmanFilter(dim_x=2, dim_z=1)\n",
    "kf.x = np.array([0,0]) # initial state x, dx\n",
    "kf.P = np.eye(2) * 1000 #initial uncertainty\n",
    "kf.R = np.array([[0.1]]) # measurement noise\n",
    "kf.Q = np.eye(2) * 0.01 # process noise\n",
    "\n",
    "dt = 1.0\n",
    "kf.F = np.array([[1,dt],\n",
    "                 [0,1]])\n",
    "\n",
    "kf.H = np.array([[1,0]])\n",
    "kf.R = np.array([[0.1]])\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    detections = model(frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KalmanFilter.__init__() got an unexpected keyword argument 'dim_y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chang\\OneDrive\\Documents\\2023-spring\\ai\\multiple-object\\Multiple_object_tracking.ipynb Cell 18\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# |---------------------------------|\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# |                                 |\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# |            TESTING              |\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# |                                 |\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# |---------------------------------|\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m f \u001b[39m=\u001b[39m KalmanFilter(dim_x\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, dim_y\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m f\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m2.\u001b[39m],    \u001b[39m# position\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                 [\u001b[39m0.\u001b[39m]])   \u001b[39m# velocity\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m f\u001b[39m.\u001b[39mF \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1.\u001b[39m,\u001b[39m1.\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 [\u001b[39m0.\u001b[39m,\u001b[39m1.\u001b[39m]])\n",
      "\u001b[1;31mTypeError\u001b[0m: KalmanFilter.__init__() got an unexpected keyword argument 'dim_y'"
     ]
    }
   ],
   "source": [
    "# |---------------------------------|\n",
    "# |                                 |\n",
    "# |            TESTING              |\n",
    "# |                                 |\n",
    "# |---------------------------------|\n",
    "\n",
    "\n",
    "f = KalmanFilter(dim_x=2, dim_y=1)\n",
    "\n",
    "f.x = np.array([[2.],    # position\n",
    "                [0.]])   # velocity\n",
    "\n",
    "f.F = np.array([[1.,1.],\n",
    "                [0.,1.]])\n",
    "\n",
    "f.H = np.array([[1.,0.]])\n",
    "\n",
    "f.P = np.array([[1000.,    0.],\n",
    "                [   0., 1000.] ])\n",
    "\n",
    "f.R = np.array([[5.]])\n",
    "\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "f.Q = Q_discrete_white_noise(dim=2, dt=0.1, var=0.13)\n",
    "\n",
    "z = get_sensor_reading()\n",
    "f.predict()\n",
    "f.update(z)\n",
    "do_something_with_estimate (f.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chang/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-14 Python-3.10.11 torch-2.0.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\chang\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached\n",
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chang\\OneDrive\\Documents\\2023-spring\\ai\\multiple-object\\Multiple_object_tracking.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     frame_no\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mprint\u001b[39m(frame_no)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m measurement \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m         [loc[frame_no][\u001b[39m0\u001b[39m]],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m         [loc[frame_no][\u001b[39m1\u001b[39m]]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     ])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m kalman\u001b[39m.\u001b[39mupdate(measurement)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Documents/2023-spring/ai/multiple-object/Multiple_object_tracking.ipynb#X36sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# print(loc)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "from scipy.linalg import block_diag\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5s')\n",
    "model.classes=[32]\n",
    "\n",
    "cap = cv2.VideoCapture('ball.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.cv.CV_FOURCC(*'DIVX')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "w = int(cap.get(3))\n",
    "h = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'MJPG'), 30, (640,480))\n",
    "\n",
    "\n",
    "\n",
    "# |---------------------------------|\n",
    "# |                                 |\n",
    "# |            Filter               |\n",
    "# |                                 |\n",
    "# |---------------------------------|\n",
    "kalman = KalmanFilter(dim_x=4, dim_z=2)\n",
    "# kalman.x = np.array([ # \n",
    "#     [0],\n",
    "#     [0],\n",
    "#     [0],\n",
    "#     [0]\n",
    "# ])\n",
    "uncertaintyInit = 500\n",
    "kalman.P=np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,1],\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1]\n",
    "]) * uncertaintyInit\n",
    "processVar = 30\n",
    "q = Q_discrete_white_noise(dim=2, dt=1.0/20.0, var=processVar)\n",
    "kalman.Q = block_diag(q,q)\n",
    "#print(kalman.Q)\n",
    "kalman.R = np.array([\n",
    "    [0.5, 0],\n",
    "    [0, 0.5]\n",
    "])\n",
    "kalman.H = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "kalman.F = np.array([\n",
    "    [1, 1.0/20.0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1.0/20.0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "frame_no = 0\n",
    "loc = []\n",
    "preds = []\n",
    "\n",
    "def midpoint(point1, point2):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    return (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "\n",
    "def printall(dict, frame, ind):\n",
    "    for i in range(ind,0,-1):\n",
    "        cv2.circle(frame, (dict[i][0]+50, dict[i][1]+50), 5, (0,0,255),-1)\n",
    "\n",
    "i = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        detections = model(frame)\n",
    "        pred = detections.pandas().xyxy\n",
    "        # print(pred)\n",
    "        \n",
    "        try:\n",
    "            # get the values\n",
    "            xmin = float(pred[0]['xmin'])\n",
    "            ymin = float(pred[0]['ymin'])\n",
    "            xmax = float(pred[0]['xmax'])\n",
    "            ymax = float(pred[0]['ymax'])\n",
    "            #print(f'{(xmin, ymin, xmax, ymax)}')\n",
    "            mid = midpoint((xmin, ymin),(xmax, ymax))\n",
    "            loc.append(mid) # add the midpoint to the vector\n",
    "            \n",
    "            frame_no+=1\n",
    "        except:\n",
    "            frame_no+= 1\n",
    "        print(frame_no)\n",
    "\n",
    "\n",
    "        \n",
    "        measurement = np.array([\n",
    "                [loc[frame_no][0]],\n",
    "                [loc[frame_no][1]]\n",
    "            ])\n",
    "        kalman.update(measurement)\n",
    "\n",
    "        # print(loc)\n",
    "        kalman.predict()\n",
    "        pred_x = int(kalman.x[0])\n",
    "        pred_y = int(kalman.x[2])\n",
    "\n",
    "        preds.append((pred_x, pred_y))\n",
    "\n",
    "        # print(\"predx:\",pred_x)\n",
    "        # print(\"predy:\",pred_y)\n",
    "        # print(\"vel:\",kalman.x[1])\n",
    "\n",
    "\n",
    "        #draw circle\n",
    "        for i in range(frame_no, 0, -1):\n",
    "            cv2.circle(frame, preds[i], 2, (0,255,0), -1)\n",
    "        #draw locations\n",
    "        for i in range(frame_no, 0, -1):\n",
    "            print(\"frame:\",frame_no)\n",
    "            print(\"drawing:\",(loc[i][0], loc[i][1]))\n",
    "            cv2.circle(frame, (loc[i][0], loc[i][1]), 5, (0,0,255), -1)\n",
    "\n",
    "\n",
    "        #cv2.imshow('frame',frame)\n",
    "        cv2.imshow(\"frame\",frame)\n",
    "        out.write(frame)\n",
    "        # draw previous points\n",
    "\n",
    "        if cv2.waitKey(1) % 0xFF == ord('s'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chang/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-14 Python-3.10.11 torch-2.0.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\chang\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "from scipy.linalg import block_diag\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5','yolov5s')\n",
    "model.classes=[32]\n",
    "\n",
    "cap = cv2.VideoCapture('ball.mp4')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#fourcc = cv2.cv.CV_FOURCC(*'DIVX')\n",
    "#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "w = int(cap.get(3))\n",
    "h = int(cap.get(4))\n",
    "\n",
    "out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'MJPG'), 30, (640,480))\n",
    "\n",
    "\n",
    "# |---------------------------------|\n",
    "# |                                 |\n",
    "# |            Filter               |\n",
    "# |                                 |\n",
    "# |---------------------------------|\n",
    "kalman = KalmanFilter(dim_x=4, dim_z=2)\n",
    "# kalman.x = np.array([ # \n",
    "#     [0],\n",
    "#     [0],\n",
    "#     [0],\n",
    "#     [0]\n",
    "# ])\n",
    "uncertaintyInit = 500\n",
    "kalman.P=np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,1],\n",
    "    [0,0,1,0],\n",
    "    [0,0,0,1]\n",
    "]) * uncertaintyInit\n",
    "processVar = 30\n",
    "q = Q_discrete_white_noise(dim=2, dt=1.0/20.0, var=processVar)\n",
    "kalman.Q = block_diag(q,q)\n",
    "#print(kalman.Q)\n",
    "kalman.R = np.array([\n",
    "    [0.5, 0],\n",
    "    [0, 0.5]\n",
    "])\n",
    "kalman.H = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "kalman.F = np.array([\n",
    "    [1, 1.0/20.0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 1.0/20.0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "frame_no = 0\n",
    "locs = []\n",
    "preds = []\n",
    "\n",
    "def midpoint(point1, point2):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    return (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "\n",
    "def printall(arr, frame, ind):\n",
    "    for i in range(ind,0,-1):\n",
    "        cv2.circle(frame, (arr[i][0], arr[i][1]), 5, (0,0,255),-1)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        detections = model(frame)\n",
    "        pred = detections.pandas().xyxy\n",
    "\n",
    "        if pred[0]['xmin'].any():\n",
    "            xmin = float(pred[0]['xmin'])\n",
    "            ymin = float(pred[0]['ymin'])\n",
    "            xmax = float(pred[0]['xmax'])\n",
    "            ymax = float(pred[0]['ymax'])\n",
    "            #print(f'{(xmin, ymin, xmax, ymax)}')\n",
    "            mid = midpoint((xmin, ymin),(xmax, ymax))\n",
    "            loc[frame_no] = mid # add the midpoint to the vector\n",
    "            # print(\"success, appended:\",mid)\n",
    "        else:\n",
    "            loc[frame_no] = loc[frame_no-1]\n",
    "            # print(\"fuck, inserted\", loc[frame_no-1], \"instead\")\n",
    "\n",
    "        measurement = np.array([\n",
    "                [loc[frame_no][0]],\n",
    "                [loc[frame_no][1]]\n",
    "            ])\n",
    "        kalman.update(measurement)\n",
    "\n",
    "        kalman.predict()\n",
    "        pred_x = int(kalman.x[0])\n",
    "        pred_y = int(kalman.x[2])\n",
    "\n",
    "        preds.append((pred_x, pred_y))\n",
    "\n",
    "        # print(\"predx:\",pred_x)\n",
    "        # print(\"predy:\",pred_y)\n",
    "        # print(\"vel:\",kalman.x[1])\n",
    "\n",
    "        for i in range(frame_no, 0, -1):\n",
    "            cv2.circle(frame, preds[i], 2, (0,255,0), -1)\n",
    "\n",
    "        for i in range(frame_no, 0, -1):\n",
    "            # print(\"frame:\",frame_no)\n",
    "            # print(\"drawing:\",(loc[i][0], loc[i][1]))\n",
    "            cv2.circle(frame, (loc[i][0], loc[i][1]), 5, (0,0,255), -1)\n",
    "\n",
    "        cv2.imshow(\"frane:\",frame)\n",
    "        out.write(frame)\n",
    "        frame_no+=1\n",
    "        if cv2.waitKey(1) % 0xFF == ord('s'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
